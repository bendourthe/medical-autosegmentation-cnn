{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9afa52c4-43bf-4899-99b9-64d05728d06c",
   "metadata": {},
   "source": [
    "# Automated Segmentation of Medical Images Using a Convolutional Neural Network\n",
    "\n",
    "\n",
    "## Objectives\n",
    "To train a Neural Network to automatically segment a selection of regions of interest (ROIs) from medical imaging data.\n",
    "\n",
    "\n",
    "## Neural Network Architecture\n",
    "The selected architecture is a Multiscale Pyramid 2D Convolutional Neural Network (Dourthe et al. (2021) [1]), which was chosen based on its reported ability to accurately extract contextual and morphological information from medical images at various scales.\n",
    "\n",
    "\n",
    "## How to Use\n",
    "\n",
    "### Requirements\n",
    "Create a new Python 3.9 environment and install requirements.txt within this environment.\n",
    "\n",
    "### Data Management\n",
    "\n",
    "#### Data Structure\n",
    "In order to allow the code below to run successfully, it is recommended to organize the training data using the following structure:\n",
    "<pre>\n",
    "main_directory\n",
    "└─ data\n",
    "   └── train\n",
    "       ├── images\n",
    "       │   └── contains all the raw images available in the training dataset (each scan axial slice represents a sample)\n",
    "       └── labels\n",
    "           └── contains all the segmentation files for the corresponding images available in the training dataset</pre>\n",
    "\n",
    "#### Data Format\n",
    "- images: DICOM format (.dcm)\n",
    "- labels: multiple formats supported: .npy, .jpg, .png\n",
    "\n",
    "#### Data Labelling\n",
    "The images and corresponding segmentation files should have the same filenames. For example, if a DICOM image is named 'axial_mri_slice_1.dcm', the corresponding segmentation file should be named 'axial_mri_slice_1.npy' or 'axial_mri_slice_1.jpg' or 'axial_mri_slice_1.png'.\n",
    "\n",
    "### Segmentation Instructions\n",
    "Segmentation files can be generated using the software of choice (3D Slicer, ImageJ, Photoshop, etc.), as long as:\n",
    "- The resulting files are in one of the supported formats (.npy, .jpg or .png)\n",
    "- The filenames match with the corresponding segmented images\n",
    "- One segmentation file is generated per slice\n",
    "\n",
    "In addition, the pixels of the segmentation file should define what specific region of interest (ROI) each pixel belongs to. For example, if a total of 4 ROIs are being segmented, every pixel that belongs to ROI #1 should have a value of 1, every pixel that belongs to ROI #2 should have a value of 2, etc. and every pixel that is not labeled (i.e. belongs to the background or other regions) should have a value of 0.\n",
    "\n",
    "### Notebook Setup\n",
    "In the Settings section:\n",
    "- Edit the different paths and filenames under the DIRECTORIES & FILENAMES section\n",
    "- Choose the right input shape under the TRAINING PARAMETERS section\n",
    "    - Optional: edit the save_checkpoint parameter to define how often to save the model during training\n",
    "- Every other parameter can be left as their original value.\n",
    "\n",
    "### Run the Notebook\n",
    "Once the Settings have been edited, run each cell of the notebook and wait for training to be completed.\n",
    "\n",
    "\n",
    "## References\n",
    "[1] Dourthe B, Shaikh N, S AP, Fels S, Brown SHM, Wilson DR, Street J, Oxland TR. Automated Segmentation of spinal Muscles from Upright Open MRI Using a Multi-Scale Pyramid 2D Convolutional Neural Network. Spine (Phila Pa 1976). 2021 Dec 15. doi: 10.1097/BRS.0000000000004308. PMID: 34919072. https://pubmed.ncbi.nlm.nih.gov/34919072/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eece207-2abb-4e28-b1ff-4f4420be6a3b",
   "metadata": {},
   "source": [
    "___\n",
    "# Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "684d76fb-e4db-4686-8e41-e700872d4f05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries successfully imported\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# System characteristics\n",
    "import psutil\n",
    "import humanize\n",
    "import GPUtil as GPU\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# Computation time monitoring\n",
    "from time import time\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "# Data processing\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Pytoolbox import\n",
    "from pytoolbox.utils import *\n",
    "from pytoolbox.dataset import *\n",
    "from pytoolbox.network import *\n",
    "from pytoolbox.loss import *\n",
    "\n",
    "print('Libraries successfully imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b516f-074a-4718-ba44-af51206820f9",
   "metadata": {},
   "source": [
    "___\n",
    "# System Characteristics\n",
    "The cell below allows you to check whether your GPU is enabled and displays the corresponding system characteristics such as memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ec83023-f7d5-4fec-940c-9e7ec0d4aed3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRAM Memory:\t\u001b[0m1.3 GB (Available)\n",
      "\n",
      "\u001b[1mGPU enabled:\t\u001b[0mFalse\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display local virtual memory (RAM)\n",
    "print(f'\\033[1mRAM Memory:\\t\\033[0m{humanize.naturalsize(psutil.virtual_memory().available)} (Available)\\n')\n",
    "\n",
    "# Check if GPU is enabled\n",
    "print(f'\\033[1mGPU enabled:\\t\\033[0m{torch.cuda.is_available()}\\n')\n",
    "\n",
    "# Setting device on GPU ('cuda') if available, if not, the device will be set to CPU ('cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# If device set to GPU ('cuda'), display device information\n",
    "if device.type == 'cuda':\n",
    "    # Collect GPU information\n",
    "    GPUs = GPU.getGPUs()\n",
    "    gpu = GPUs[0]\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(f'\\033[1mDevice Name:\\t\\033[0m{torch.cuda.get_device_name(0)}')\n",
    "    print(f'\\033[1mMemory Details:\\t\\033[0m{gpu.memoryTotal/1000:3.1f} GB '\n",
    "          f'(Total)\\t\\t{gpu.memoryUsed/1000:3.1f} GB ({gpu.memoryUtil*100:.0f}% Used) '\n",
    "          f'\\t{gpu.memoryFree/1000:4.1f} GB ({100-gpu.memoryUtil*100:.0f}% Free)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c08281d-c09e-4370-9432-c5479a7b917e",
   "metadata": {},
   "source": [
    "___\n",
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "344421e2-ea9b-408b-b9db-4483b8029641",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings successfully defined\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "# DIRECTORIES & FILENAMES #\n",
    "###########################\n",
    "\n",
    "# Define path towards data directory\n",
    "main_dir = 'C:/Users/username/project_name/data'\n",
    "\n",
    "# Define paths towards training data\n",
    "train_dicoms_path = main_dir + '/train/images'\n",
    "train_labels_path = main_dir + '/train/labels'\n",
    "\n",
    "# Define path towards directory where the trained model and training history will be saved\n",
    "model_export_path = 'trained_models'\n",
    "# Check if the corresponding directory exists, if not, create it\n",
    "if not os.path.exists(os.path.abspath(model_export_path)):\n",
    "    os.mkdir(os.path.abspath(model_export_path))\n",
    "\n",
    "# Define filename to save trained model state \n",
    "#   NOTE: the number of epochs will be added at the end of the model filename when saved\n",
    "#   to reflect training state\n",
    "model_filename = 'autoseg_mri_model'\n",
    "\n",
    "\n",
    "#######################\n",
    "# TRAINING PARAMETERS #\n",
    "#######################\n",
    "\n",
    "# Define input shape (i.e. number of pixels along x- or y-axis)\n",
    "#   NOTE: Only one value is needed as input images are either squares,\n",
    "#   or will be resized to squares within the data loader\n",
    "input_shape = 256\n",
    "\n",
    "# Define data augmentation parameters\n",
    "augment_params = {'aug_prob': 0.5,\n",
    "                  'rotation': True,\n",
    "                  'rot_range': 5,\n",
    "                  'warping': True,\n",
    "                  'warp_range': 0.8,\n",
    "                  'cropping': True,\n",
    "                  'crop_range': [0.5, 0.8]}\n",
    "\n",
    "# Define computing parameters\n",
    "# (i.e. how many samples are processed simultaneously and how many parallelized computers/GPUs are used)\n",
    "batch_size = 5\n",
    "num_workers = 0\n",
    "pin_memory = False\n",
    "\n",
    "# Define number of epochs (i.e. iterations) used to complete training\n",
    "epochs = 500\n",
    "\n",
    "# Define learning parameters\n",
    "learning_rate = 0.0001\n",
    "dropout_rate = 0.3\n",
    "\n",
    "# Define checkpoints parameters\n",
    "print_checkpoint = 10          # Defines how often will a print statement be displayed during training (in number of epochs)\n",
    "save_checkpoint = 100          # Defines how often will the model state be save (in number of epochs)\n",
    "\n",
    "###############################\n",
    "# DATA VISUALIZATION SETTINGS #\n",
    "###############################\n",
    "\n",
    "# Dark mode\n",
    "dark_mode = True\n",
    "\n",
    "# Define Jupyter theme based on dark mode\n",
    "# list available themes\n",
    "# onedork | grade3 | oceans16 | chesterish | monokai | solarizedl | solarizedd\n",
    "if dark_mode:\n",
    "    jtplot.style(theme='chesterish')\n",
    "else:\n",
    "    jtplot.style(theme='grade3')\n",
    "\n",
    "print('Settings successfully defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3820f219-70c4-4a6e-807b-e9eb39e9317c",
   "metadata": {},
   "source": [
    "___\n",
    "# Dataset Pre-Visualization\n",
    "The cell below allows you to load and display the first batch of the dataset after being passed through the data loader.\n",
    "\n",
    "NOTE: This data loader includes data augmentation techniques that are implemented on-the-fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "016a64be-3980-4f9f-8017-a435cface925",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:/Users/username/project_name/data/train/images'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m###################\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# DATASET LOADING #\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m###################\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Generate dataset\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mProcessedDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dicoms_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[0;32m      9\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39mnum_workers, pin_memory\u001b[38;5;241m=\u001b[39mpin_memory)\n",
      "File \u001b[1;32m~\\IDrive-Sync\\Work\\Coding\\Github\\spine-mri-autosegmentation\\pytoolbox\\dataset.py:112\u001b[0m, in \u001b[0;36mProcessedDataset.__init__\u001b[1;34m(self, train_dicoms_path, train_labels_path, input_shape, augment_params)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugment_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrop_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugment_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrop_range\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    105\u001b[0m                                                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugment_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrop_range\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m])        \n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m#########################\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# DICOMS PRE-PROCESSING #\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m#########################\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Define list of scans\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdicom_files_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dicoms_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Initialize empty list that will contain all training images\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# (array of shape = (number of samples, x-axis resolution, y-axis resolution)\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_list \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:/Users/username/project_name/data/train/images'"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# DATASET LOADING #\n",
    "###################\n",
    "\n",
    "# Generate dataset\n",
    "train_data = ProcessedDataset(train_dicoms_path, train_labels_path, input_shape=input_shape, augment_params=augment_params)\n",
    "\n",
    "# Load dataset\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "# Grab the first sample\n",
    "for img, lab in train_loader:\n",
    "    break\n",
    "\n",
    "# Print shape of the first batch\n",
    "print('FIRST BATCH SHAPES:')\n",
    "print(f'\\tImages:\\t{img.shape}')\n",
    "print(f'\\tLabels:\\t{lab.shape}')\n",
    "\n",
    "# Generate grids for low and high resolution images from first batch\n",
    "#   Resize to 3D array and convert to numpy.ndarray \n",
    "img_arr = img.numpy()\n",
    "lab_arr = lab.numpy()\n",
    "\n",
    "#   Initialize grid arrays with a column vector of zeros\n",
    "img_grid = np.zeros((img_arr.shape[3], 1))\n",
    "lab_grid = np.zeros((lab_arr.shape[2], 1))\n",
    "#   Isolate the indices of 5 evenly distanced labeled frames and \n",
    "for i in range(batch_size):\n",
    "    # Stack each new image along the x-axis to generate a grid of 5 stacked images\n",
    "    img_grid = np.hstack([img_grid, img_arr[i, 0, :, :]])\n",
    "    lab_grid = np.hstack([lab_grid, lab_arr[i, :, :]])\n",
    "\n",
    "# Display grids\n",
    "plt.figure(figsize=(26, 6))\n",
    "plt.imshow(img_grid, cmap='gray')\n",
    "plt.imshow(lab_grid, alpha=0.6, cmap='cubehelix')\n",
    "ax = plt.gca()\n",
    "ax.axes.get_xaxis().set_visible(False)\n",
    "ax.axes.get_yaxis().set_visible(False)\n",
    "plt.title('First Batch of Augmented Segmented Images', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a911c4c-79d4-46fa-b3c4-c268fd74e601",
   "metadata": {},
   "source": [
    "___\n",
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b94a87c-22b1-4127-b729-88483839101d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCompleted epochs:\u001b[0m    1/500 | \u001b[1mDice loss:\u001b[0m 2.000 | \u001b[1mMean dice score:\u001b[0m 0.000 | \u001b[1mTime elapsed:\u001b[0m  0 hrs  3 mins  2 secs\n",
      "\u001b[1mCompleted epochs:\u001b[0m   10/500 | \u001b[1mDice loss:\u001b[0m 1.426 | \u001b[1mMean dice score:\u001b[0m 0.287 | \u001b[1mTime elapsed:\u001b[0m  0 hrs 30 mins 27 secs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 70>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Pass sample through model\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m stage1_output, stage2_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Calculate sample loss\u001b[39;00m\n\u001b[0;32m     88\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(stage1_output, stage2_output, lab, device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\autoseg_mri_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\autoseg_mri_env\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:150\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataParallel.forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids:\n\u001b[1;32m--> 150\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mbuffers()):\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\autoseg_mri_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\IDrive-Sync\\Work\\Coding\\Github\\spine-mri-autosegmentation\\code\\notebooks\\..\\pytoolbox\\network.py:275\u001b[0m, in \u001b[0;36mMultiScalePyramid.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    273\u001b[0m stage1_input \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mupsample(inputs, (\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_shape\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m), \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_shape\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m)        \n\u001b[0;32m    274\u001b[0m \u001b[38;5;66;03m#   Generate stage 1 output by passing downsampled input through the 3D FCN\u001b[39;00m\n\u001b[1;32m--> 275\u001b[0m stage1_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage1_input\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[0;32m    276\u001b[0m \u001b[38;5;66;03m#   Upsample output back to original size\u001b[39;00m\n\u001b[0;32m    277\u001b[0m stage1_output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mupsample(stage1_output, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_shape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_shape), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\autoseg_mri_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\IDrive-Sync\\Work\\Coding\\Github\\spine-mri-autosegmentation\\code\\notebooks\\..\\pytoolbox\\network.py:198\u001b[0m, in \u001b[0;36mMultiLabelVNet.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m dec_layer1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(dec_layer1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_rate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)        \n\u001b[0;32m    196\u001b[0m up1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_conv1(dec_layer1)\n\u001b[1;32m--> 198\u001b[0m dec_layer2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mup1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_layer3\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m up1\n\u001b[0;32m    199\u001b[0m dec_layer2 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(dec_layer2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_rate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)        \n\u001b[0;32m    200\u001b[0m up2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_conv2(dec_layer2)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\autoseg_mri_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\autoseg_mri_env\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\autoseg_mri_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\autoseg_mri_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\autoseg_mri_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    441\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    442\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "######################\n",
    "# DATASET DEFINITION #\n",
    "######################\n",
    "\n",
    "# Generate dataset\n",
    "train_data = ProcessedDataset(train_dicoms_path, train_labels_path, augment_params=augment_params)\n",
    "\n",
    "# Load dataset\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "# Grab the first sample\n",
    "for img, lab in train_loader:\n",
    "    break\n",
    "    \n",
    "# Define model parameters\n",
    "input_dim = img.shape[1]\n",
    "input_shape = img.shape[2]\n",
    "\n",
    "########################\n",
    "# MODEL INITIALIZATION #\n",
    "########################\n",
    "\n",
    "# Create initialization function\n",
    "def init(model):\n",
    "    if isinstance(model, nn.Conv3d) or isinstance(model, nn.ConvTranspose3d):\n",
    "        nn.init.kaiming_normal(model.weight.data, 0.25)\n",
    "        nn.init.constant(model.bias.data, 0)\n",
    "        \n",
    "# Create instance of the multi-scale pyramid model\n",
    "model = MultiScalePyramid(num_labels=num_labels, input_shape=input_shape, training=True)\n",
    "\n",
    "# Initialize model\n",
    "model.apply(init)\n",
    "\n",
    "############################\n",
    "# OPTIMIZER INITIALIZATION #\n",
    "############################\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define learning rate decay\n",
    "lr_decay = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1000], gamma=0.1)\n",
    "\n",
    "##################\n",
    "# MODEL TRAINING #\n",
    "##################\n",
    "\n",
    "# Assign device to 'cuda' if available, or to 'cpu' otherwise\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Enable cudnn benchmark, which allows cudnn to look for the optimal set of algorithms for the current device configuration\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Implements data parallelism at the module level\n",
    "model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Assign model to available device ('cuda' or 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Generate instance of the loss function\n",
    "criterion = MultiClassDiceLoss(num_labels=num_labels, input_shape=input_shape)\n",
    "\n",
    "# Initialize mean epoch loss tracker (-> one mean loss value per epoch)\n",
    "mean_epoch_loss = []\n",
    "\n",
    "# Initialize time tracker\n",
    "start_time = time()\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    # Update learning rate decay (will only update by gamma when epoch milestone is reached)\n",
    "    lr_decay.step()\n",
    "    \n",
    "    # Initialize mean sample loss tracker (-> one mean loss value per sample)\n",
    "    mean_sample_loss = []\n",
    "    \n",
    "    # Loop through training samples\n",
    "    for img, lab in train_loader:\n",
    "        \n",
    "        # Assign image to available device ('cuda' or 'cpu')\n",
    "        img = img.to(device)\n",
    "\n",
    "        # Pass sample through model\n",
    "        stage1_output, stage2_output = model(img)\n",
    "        \n",
    "        # Calculate sample loss\n",
    "        loss = criterion(stage1_output, stage2_output, lab, device)\n",
    "        \n",
    "        # Append loss to mean sample loss tracker\n",
    "        mean_sample_loss.append(loss.item())\n",
    "        \n",
    "        # Update model parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Append loss to mean epoch loss tracker\n",
    "    mean_epoch_loss.append(loss.item())    \n",
    "    \n",
    "    # Print update statement for tracking\n",
    "    if epoch == 1 or epoch%print_checkpoint == 0:\n",
    "        current_time = time() - start_time\n",
    "        print(f'\\033[1mCompleted epochs:\\033[0m {epoch:4.0f}/{epochs} | '\n",
    "              f'\\033[1mDice loss:\\033[0m {loss.item():4.3f} | '\n",
    "              f'\\033[1mMean dice score:\\033[0m {(2-loss.item())/2:4.3f} | '\n",
    "              f'\\033[1mTime elapsed:\\033[0m {current_time//3600:2.0f} hrs '\n",
    "              f'{(current_time - current_time//3600*3600)//60:2.0f} mins '\n",
    "              f'{current_time%60:2.0f} secs')\n",
    "\n",
    "    # Save model every checkpoint\n",
    "    if epoch%save_checkpoint == 0:\n",
    "        #   Define current model state using dictionary\n",
    "        model_state = {'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "        # Save model checkpoint\n",
    "        torch.save(model_state, model_export_path + '/' + model_filename + '_' + str(model_state['epoch']) + '.pt')\n",
    "        #   Print save statement\n",
    "        print(f'\\n\\tCheckpoint -> Model saved at {epoch:4.0f}/{epochs} epochs\\n')\n",
    "\n",
    "# Define final model state using dictionary\n",
    "model_state = {'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "\n",
    "# Generate final loss history DataFrame\n",
    "loss_history = pd.DataFrame(mean_epoch_loss, columns=['Dice loss'])\n",
    "\n",
    "# Save final model and loss history\n",
    "torch.save(model_state, model_export_path + '/' + model_filename + '_' + str(model_state['epoch']) + '.pt')\n",
    "loss_history.to_csv(model_export_path + '/' + model_filename + '_history_' + str(epoch) + '.csv')\n",
    "\n",
    "# Plot training history\n",
    "loss_history.plot(figsize=(24, 6))\n",
    "plt.legend(loc='upper right', fontsize=15)\n",
    "plt.xlabel('epoch', fontsize=20)\n",
    "plt.ylabel('Dice loss', fontsize=20)\n",
    "plt.title('Training history', fontsize=30)\n",
    "plt.show()\n",
    "\n",
    "# Print total computing time\n",
    "total_time = time() - start_time\n",
    "print(f'\\033[1m\\nTotal computing time:\\033[0m {total_time//3600:2.0f} hrs '\n",
    "      f'{(total_time - total_time//3600*3600)//60:2.0f} mins '\n",
    "      f'{total_time%60:2.0f} secs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoseg_mri_env",
   "language": "python",
   "name": "autoseg_mri_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
